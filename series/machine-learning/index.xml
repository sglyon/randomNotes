<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Random Notes</title>
    <link>http://notes.spencerlyon.com/series/machine-learning/</link>
    <description>Recent content in Machine Learning on Random Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://notes.spencerlyon.com/series/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reinforcement Learning</title>
      <link>http://notes.spencerlyon.com/2016/06/29/reinforcement-learning/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/reinforcement-learning/</guid>
      <description>

&lt;p&gt;These notes mostly follow Sutton, R. S., &amp;amp; Barto, A. G. (2015). Reinforcement Learning: An Introduction (2 Draft, Vol. 9). &lt;a href=&#34;http://doi.org/10.1109/TNN.1998.712192&#34;&gt;http://doi.org/10.1109/TNN.1998.712192&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The notes themselves might be interpreted as a macro-economist&amp;rsquo;s view of reinforcement learning (RL). I will cast many objects/ideas from the book into the domain-specific framework I am familiar with.&lt;/p&gt;

&lt;h1 id=&#34;what-is-reinforcement-learning&#34;&gt;What is reinforcement learning?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; is a branch of machine learning that aims to learn by doing.&lt;/p&gt;

&lt;h1 id=&#34;basic-strcture&#34;&gt;Basic strcture&lt;/h1&gt;

&lt;p&gt;The basic structure of the RL problem is similar to a dynamic programming (DP) problem. Both summarize the environment an agent faces via a state variable $s$ that is a member of the state space $S$. Agents must choose actions from an action space $A(s)$. The dependence on $s$ takes into account any state-dependent constraints that specify which actions are feasible from that given state. A generic element of the space $A$ is called an action and is denoted $a \in A$.&lt;/p&gt;

&lt;p&gt;In both RL and DP, algorithms are constructed to choose policies $\pi: S -&amp;gt; \Delta(A)$ and approximate or evaluate the value of such policies. The notaiton $\Delta(A)$ is a simplex or probability distribution over $A$. When we write $\pi(a | s)$ the probability of choosing $a$ in state $s$ under the policy $\pi$. In most economic models policies are degenerate, meaning that they specify a single action with probablity 1 for each state. In this case we may write $a = \pi(s)$ as the policy.&lt;/p&gt;

&lt;p&gt;In DP these values are often expresesd as a value function $V: S -&amp;gt; R$. This is known as a &lt;strong&gt;state-value&lt;/strong&gt; function.&lt;/p&gt;

&lt;p&gt;In RL values are either expressed using $V$, or using a &lt;strong&gt;action-value&lt;/strong&gt; function $Q: S \times A -&amp;gt; R$.&lt;/p&gt;

&lt;p&gt;Let $\gamma \in [0, 1]$ be a discount factor.&lt;/p&gt;

&lt;p&gt;Timing in Sutton and Barto is such that in time $t$ the agent sees state $S_t \in S$, chooses action $A_t \in A(S&lt;em&gt;t)$, then recieves a &lt;strong&gt;reward&lt;/strong&gt; $R&lt;/em&gt;{t+1}$, and finally observes a new state $S_{t+1} \in S$.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Return&lt;/strong&gt; $G_t \in R$ is the sum of discounted rewards:&lt;/p&gt;

&lt;!-- NOTE: for hugo we need to escape the `_` for some reason... --&gt;

&lt;p&gt;$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$&lt;/p&gt;

&lt;p&gt;We can write the value of following a particular policy $\pi$ starting in a state $s$ using state-value functions as:&lt;/p&gt;

&lt;p&gt;$$v_{\pi}(s) \equiv E_{\pi} \left[G_t | S_t = s \right] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right].$$&lt;/p&gt;

&lt;p&gt;Likewise we can write the action-value of following policy $\pi$ from state $s$ and action $a$ as&lt;/p&gt;

&lt;p&gt;$$q_{\pi}(s, a) \equiv E_{\pi} \left[G_t | S_t = s A_t = a\right] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right].$$&lt;/p&gt;

&lt;p&gt;We define the optimal state-value function as $v(s) \equiv \max_{\pi} v_{\pi}(s)$ and the optimal action-value function as $q(s, a) \equiv \max_{\pi} q_{\pi}(s, a)$. We can obtain $v$ from $q$ as $v = \max_{a} q(s, a)$. Thus, knowing $q$ gives more information than knowing $v$: $q$ gives the optimal value of taking &lt;em&gt;any&lt;/em&gt; action from state $s$ and $v$ gives the optimal value of taking the &lt;em&gt;optimal&lt;/em&gt; value from state $s$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>