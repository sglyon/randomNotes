<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability  Statistics on Random Notes</title>
    <link>http://notes.spencerlyon.com/series/probability--statistics/index.xml</link>
    <description>Recent content in Probability  Statistics on Random Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <atom:link href="http://notes.spencerlyon.com/series/probability--statistics/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Probability distributions</title>
      <link>http://notes.spencerlyon.com/2015/12/28/probability-distributions/</link>
      <pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2015/12/28/probability-distributions/</guid>
      <description>

&lt;p&gt;Some probability distributions that are useful (usually to economists) for one reason or another.&lt;/p&gt;

&lt;h3 id=&#34;pareto-distribution&#34;&gt;Pareto Distribution&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareto_distribution&#34;&gt;Pareto distribution&lt;/a&gt; has a simple CDF: $G(x) = 1 - \underline{x}^{\gamma}x^{-\gamma}$, where $\underline{x}$ satisfies $G(\underline{x}) = 0$ and $\gamma$ governs the variance.&lt;/p&gt;

&lt;p&gt;A useful property of the Pareto distribution is that when it is truncated, the truncated CDF is the same as the original, except that $\underline{x}$ is moved up.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes on Kalman Filter</title>
      <link>http://notes.spencerlyon.com/1/01/01/notes-on-kalman-filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/1/01/01/notes-on-kalman-filter/</guid>
      <description>

&lt;h1 id=&#34;kalman-filter&#34;&gt;Kalman Filter&lt;/h1&gt;

&lt;p&gt;The Kalman filter is a vital tool in any macro-economist&amp;rsquo;s (and more generally any modelers) toolbox. A filtering problem is loosely described by the use of a history of observed information to infer information about the history of some other unobservable variable (state). The Kalman filter is a recursive filter in the sense that if we have a best guess for the state value for the previous period, $t-1$, then the period $t$ observation in conjunction with the best guess for the state value in the previous period is sufficient to provide a best guess for the state value in this period.&lt;/p&gt;

&lt;p&gt;The model that we will present in conjunction with the Kalman filter is called a Gaussian linear state space model and is typically described by the following two equations:&lt;/p&gt;

&lt;p&gt;$$y_t = C x_t + D \varepsilon_t$$
$$x&lt;em&gt;t = A x&lt;/em&gt;{t-1} + B \eta_t$$&lt;/p&gt;

&lt;p&gt;The matrices $(A, B, C, D)$ are all known and the model noise, $\varepsilon_t$ and $\eta_t$, are both identically and independently distributed as standard normals. The first equation is often referred to as the &lt;em&gt;measurement equation&lt;/em&gt; because the history of $y^t$ is observable to the agent at period $t$.  The second equation is known as the state evolution equation, and is fundamentally different than the measurement equation because $s^t$ is unobservable. The fact that one equation is observable while the other is not and the Markov properties of the model put this model into a broader class of models known as &lt;em&gt;hidden Markov models&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;derivation&#34;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;I will now present the derivation of the Kalman filter. As mentioned earlier, we would like to infer information about the states, $x_t$, using information that we observe $y_t$. More specifically, we would like to infer the value of $x_t$ given a history of $y^t$. We begin by having some initial condition on $x_0$ &amp;ndash; This condition can be either a distribution &lt;em&gt;or&lt;/em&gt; simply a value. We will denote the estimated values of the state using a hat i.e. write $\hat{x}$&lt;/p&gt;

&lt;p&gt;The recursive nature of the Kalman filter means that each period we enter the period with a best guess for the value of the state which we will call the predicted state, call it $\hat{x}_{t | t-1}$. This predicted state is found through&lt;/p&gt;

&lt;p&gt;$$ E_t[x_t] = E&lt;em&gt;t \left[ E&lt;/em&gt;{t-1} \left[x_t \right] \right]$$
$$ = E&lt;em&gt;t \left[ E&lt;/em&gt;{t-1} \left[A x_{t-1} + B \varepsilon_t \right] \right]$$
$$ = E&lt;em&gt;t \left[ A \hat{x}&lt;/em&gt;{t-1 | t-1} + B \varepsilon&lt;em&gt;t \right] = A \hat{x}&lt;/em&gt;{t-1 | t-1}$$&lt;/p&gt;

&lt;p&gt;where $\hat{x}&lt;em&gt;{t-1 | t-1}$ is defined as our best estimate of the state conditional on the information up to period $t-1$. Our goal is to establish an unbiased estimator, $\hat{x}&lt;/em&gt;{t | t}$, for the state $x_t$ and to minimize the squared error at every period where we define the error as $e_t := (x&lt;em&gt;t - \hat{x}&lt;/em&gt;{t|t})$. Others have found (and justified) a solution of the form&lt;/p&gt;

&lt;p&gt;$$ \hat{x}&lt;em&gt;{t | t} = \hat{x}&lt;/em&gt;{t | t-1} + K (y&lt;em&gt;t - C \hat{x}&lt;/em&gt;{t | t-1})$$&lt;/p&gt;

&lt;p&gt;We refer to $(y&lt;em&gt;t - C \hat{x}&lt;/em&gt;{t | t-1})$ as the innovation or residual because it is the difference between what we would observe with no noise and what we actually observe. Taking the derivative of the square errors with respect to $K_t$ reveals the value of $K&lt;em&gt;t$ which minimizes the distance between $x$ and $\hat{x}&lt;/em&gt;{t | t}$.&lt;/p&gt;

&lt;p&gt;$$K_t = &amp;hellip; $$&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;Fill these in more adequately later&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Quant-Econ&lt;/li&gt;
&lt;li&gt;D.B.O. Anderson and J.B Moore. Optimal Filtering. Dover Publications, 1979.
*&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Notes on MCMC Methods</title>
      <link>http://notes.spencerlyon.com/1/01/01/notes-on-mcmc-methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/1/01/01/notes-on-mcmc-methods/</guid>
      <description>

&lt;h1 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h1&gt;

&lt;p&gt;The key idea behind the MCMC algorithms is that under certain conditions, Markov chains have a stationary distribution. If we can build a Markov chain whose stationary distribution is the distribution that we would like to sample from then it is relatively easy to get draws from this distribution by simulating lots of points and then randomly drawing from them.&lt;/p&gt;

&lt;h1 id=&#34;metropolis-hastings&#34;&gt;Metropolis-Hastings&lt;/h1&gt;

&lt;p&gt;Both the Metropolis and Gibbs algorithms are special cases of the Metropolis-Hastings algorithm. It has &amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;metropolis&#34;&gt;Metropolis&lt;/h1&gt;

&lt;p&gt;As previously mentioned, the Metropolis algorithm is a special case of Metropolis-Hastings &amp;ndash; In particular, it is the case in which we have a symmetric proposal density (People often use random walks)&lt;/p&gt;

&lt;h1 id=&#34;gibbs&#34;&gt;Gibbs&lt;/h1&gt;

&lt;p&gt;Gibbs sampling uses closed form expressions for conditionals such that we accept each draw.&lt;/p&gt;

&lt;h1 id=&#34;coupling-from-the-past&#34;&gt;Coupling From the Past&lt;/h1&gt;

&lt;p&gt;Imagine we have a Markovian process ${ X_t }$ whose stationary distribution we would like to draw from. One natural way to draw from this process is to start at some $x_0$ and simulate the process forward until it &amp;laquo;converges&amp;raquo; in some sense to the stationary distribution &amp;ndash; Remember under certain conditions (such as irreducibility etc&amp;hellip;) Markov processes are guaranteed to converge to their stationary distribution as $t \rightarrow \infty$. While intuitive, this approach has several short comings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. You are never drawing from the *exact* stationary distribution
2. It is hard to determine rates of convergence, so the number of periods you choose for &amp;quot;convergence&amp;quot; is somewhat arbitrary and is done using guess work.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to deal with this, we can take advantage of a very simple idea. Let&amp;rsquo;s start a process at period $-\infty$ and simulate it forward until period 0. At period 0, it should have converged to the stationary distribution. In continuous state spaces it is more difficult, so I will explain it for discrete spaces. Imagine we have $N$ states in our Markov chain then at period $-T$, it has to have arrived at one of the $N$ states. Start $N$ processes (one at each state) at period $-T$ and simulate them forward until period 0 (with the same set of shocks!). If all of these processes have converged to a single state then we know that our original process would have also converged to that state and thus the process is now in its stationary distribution (because we have &amp;laquo;simulated&amp;raquo; the process for an infinite number of periods). If it hasn&amp;rsquo;t converged then try again with a larger $T$ &amp;ndash; These processes are guaranteed to &amp;laquo;coalesce&amp;raquo; in finite number of states.&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;John Stachurski&amp;rsquo;s coupling from the past papers&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>