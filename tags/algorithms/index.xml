<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on Random Notes</title>
    <link>http://notes.spencerlyon.com/tags/algorithms/index.xml</link>
    <description>Recent content in Algorithms on Random Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <atom:link href="http://notes.spencerlyon.com/tags/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reinforcement Learning Intro</title>
      <link>http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/</guid>
      <description>

&lt;p&gt;This is part 1 in the reinforcement learning for economists notes. For a list of all entries in the series go &lt;a href=&#34;http://notes.spencerlyon.com/series/reinforcement-learning&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These notes mostly follow Sutton, R. S., &amp;amp; Barto, A. G. (2015). Reinforcement Learning: An Introduction (2 Draft, Vol. 9). &lt;a href=&#34;http://doi.org/10.1109/TNN.1998.712192&#34;&gt;http://doi.org/10.1109/TNN.1998.712192&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The notes themselves might be interpreted as a macro-economist&amp;rsquo;s view of reinforcement learning (RL). I will cast many objects/ideas from the book into the domain-specific framework I am familiar with.&lt;/p&gt;

&lt;h1 id=&#34;what-is-reinforcement-learning&#34;&gt;What is reinforcement learning?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; is a branch of machine learning that aims to learn by doing.&lt;/p&gt;

&lt;h1 id=&#34;basic-strcture&#34;&gt;Basic strcture&lt;/h1&gt;

&lt;p&gt;The basic structure of the RL problem is similar to a dynamic programming (DP) problem. Both summarize the environment an agent faces via a state variable $s$ that is a member of the state space $S$. Agents must choose actions from an action space $A(s)$. The dependence on $s$ takes into account any state-dependent constraints that specify which actions are feasible from that given state. A generic element of the space $A$ is called an action and is denoted $a \in A$.&lt;/p&gt;

&lt;p&gt;In both RL and DP, algorithms are constructed to choose policies $\pi: S -&amp;gt; \Delta(A)$ and approximate or evaluate the value of such policies. The notaiton $\Delta(A)$ is a simplex or probability distribution over $A$. When we write $\pi(a | s)$ the probability of choosing $a$ in state $s$ under the policy $\pi$. In most economic models policies are degenerate, meaning that they specify a single action with probablity 1 for each state. In this case we may write $a = \pi(s)$ as the policy.&lt;/p&gt;

&lt;p&gt;In DP these values are often expresesd as a value function $V: S -&amp;gt; R$. This is known as a &lt;strong&gt;state-value&lt;/strong&gt; function.&lt;/p&gt;

&lt;p&gt;In RL values are either expressed using $V$, or using a &lt;strong&gt;action-value&lt;/strong&gt; function $Q: S \times A -&amp;gt; R$.&lt;/p&gt;

&lt;p&gt;Let $\gamma \in [0, 1]$ be a discount factor.&lt;/p&gt;

&lt;p&gt;Timing in Sutton and Barto is such that in time $t$ the agent sees state $S_t \in S$, chooses action $A_t \in A(S&lt;em&gt;t)$, then recieves a &lt;strong&gt;reward&lt;/strong&gt; $R&lt;/em&gt;{t+1}$, and finally observes a new state $S_{t+1} \in S$.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Return&lt;/strong&gt; $G_t \in R$ is the sum of discounted rewards:&lt;/p&gt;

&lt;!-- NOTE: for hugo we need to escape the `_` for some reason... --&gt;

&lt;p&gt;$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$&lt;/p&gt;

&lt;p&gt;We can write the value of following a particular policy $\pi$ starting in a state $s$ using state-value functions as:&lt;/p&gt;

&lt;p&gt;$$v_{\pi}(s) \equiv E_{\pi} \left[G_t | S_t = s \right] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right].$$&lt;/p&gt;

&lt;p&gt;Likewise we can write the action-value of following policy $\pi$ from state $s$ and action $a$ as&lt;/p&gt;

&lt;p&gt;$$q_{\pi}(s, a) \equiv E_{\pi} \left[G_t | S_t = s A_t = a\right] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right].$$&lt;/p&gt;

&lt;p&gt;We define the optimal state-value function as $v(s) \equiv \max_{\pi} v_{\pi}(s)$ and the optimal action-value function as $q(s, a) \equiv \max_{\pi} q_{\pi}(s, a)$. We can obtain $v$ from $q$ as $v = \max_{a} q(s, a)$. Thus, knowing $q$ gives more information than knowing $v$: $q$ gives the optimal value of taking &lt;em&gt;any&lt;/em&gt; action from state $s$ and $v$ gives the optimal value of taking the &lt;em&gt;optimal&lt;/em&gt; value from state $s$.&lt;/p&gt;

&lt;h1 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h1&gt;

&lt;p&gt;The class of RL algorithms can be understood by first defining some terms from DP.&lt;/p&gt;

&lt;p&gt;Given a policy $\pi$, &lt;strong&gt;policy evaluation&lt;/strong&gt; is the process by which the state-or-action-value function is computed from $\pi$. This typically happens via some sort of backup operation. In DP we use a &lt;strong&gt;full-backup&lt;/strong&gt; where &amp;laquo;each iteration of iterative policy evaluation backs up the value of every state once to produce the new approximate value function&amp;raquo;. Economists call this full-backup one iteration on the Bellman equation.&lt;/p&gt;

&lt;p&gt;In a control problem, after evaluating a policy we typically seek to improve it. &lt;strong&gt;policy-improvement&lt;/strong&gt; is often implemented by selecting a &lt;strong&gt;greedy&lt;/strong&gt; policy state by state. That is, a new policy $\pi&amp;rsquo;$ is obtained from $\pi$ by selecting $\pi&amp;rsquo;(s) = \text{argmax}_a q_{\pi}(s, a)$.&lt;/p&gt;

&lt;h2 id=&#34;dp-algorithms&#34;&gt;DP Algorithms&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Generalized policy iteration (GPI)&lt;/strong&gt; is a major concept in the Sutton and Barto book. The core idea is alternating between policy evaluation and policy improvement. Policy evaluation drives the estimated value function towards the value of the policy while policy improvement improecs the policy with respect to the estimated value function. These two operations are competing in some sense as each one creates a moving target for the other. If both evaluation and improvement components have stabalized, then the value and policy functions must be optimal. The value function only converges when it is consistent with the current policy and the policy function only converges when it is optimal with respect to the current value function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy-iteration&lt;/strong&gt; is an example of GPI where we alternate on full policy evaluation and full policy improvment. The evaluation step provides the value of the current policy. The improvement step obtains a new policy, taking into account the values associated with the current policy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value-iteration&lt;/strong&gt; is another example of GPI where one iteration of the bellman operator is applied as a partial policy evaluation, followed by a full policy improvement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Asynchronous DP algorithms&lt;/strong&gt; are modifications of policy or value iteration where you do not perform a full backup of the policy or value functions on each iteration (meaning you don&amp;rsquo;t update the policy and value functions for all states in every iteration). Instead, these methods vary in which states they update, using whatever values for other states happen to be available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monte Carlo methods</title>
      <link>http://notes.spencerlyon.com/2016/06/29/monte-carlo-methods/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/monte-carlo-methods/</guid>
      <description>

&lt;p&gt;This is part 2 in the reinforcement learning for economists notes. For part 1 see &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt;. For a list of all entries in the series go &lt;a href=&#34;http://notes.spencerlyon.com/series/reinforcement-learning&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;monte-carlo-methods&#34;&gt;Monte Carlo methods&lt;/h2&gt;

&lt;p&gt;As defined in &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt; &lt;em&gt;GPI&lt;/em&gt; encompasses the core ideas for all the RL algorithms in this book. One family of such algorithms is Monte Carlo (MC) methods. One distinguishing feature of these methods is that they can be implemented in a completely &lt;strong&gt;model-free&lt;/strong&gt; way. This means that to apply them we need to know nothing about the dynamics of the model; we simply need to be able to observe sequences of states, actions, and rewards.&lt;/p&gt;

&lt;h3 id=&#34;prediciton-methods&#34;&gt;Prediciton methods&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo prediciton&lt;/strong&gt; is a method for learning the state-value function for a given policy. The algorithm is given by:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/first_visit_mc_prediction.png&#34; alt=&#34;MC prediciton&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that the MC prediction algorithm does not update any estimates of the value function until &lt;em&gt;after&lt;/em&gt; the entire episode is realized. In this sense we would say that the method does not &lt;strong&gt;bootstrap&lt;/strong&gt;, meaning it does not use approximations of the value in other states to update the value in a particular state (value iteration is bootstrapping).&lt;/p&gt;

&lt;h3 id=&#34;control-methods&#34;&gt;Control methods&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo control&lt;/strong&gt; is an iterative algorithm where each iteration contains a Monte Carlo prediction step followed by a policy improvment step. The improvement step is simply done by making the policy greedy with the predicted value function.&lt;/p&gt;

&lt;p&gt;Some technical conditions must be satisfied to ensure convergence of this naive algorithm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;episodes have exploring starts (meaning that they don&amp;rsquo;t attempt to be greedy or optimal at the start)&lt;/li&gt;
&lt;li&gt;There are an infinite number of episodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are ok theoretically, but prohibative computationally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo ES&lt;/strong&gt; is an algorithm that does away with the assumption that you need an infinite number of episodes. It proceeds as&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/monte_carlo_es.png&#34; alt=&#34;MC ES&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that we don&amp;rsquo;t need the assumption of an infinite number of episodes because we don&amp;rsquo;t do a full policy evaluation each step.&lt;/p&gt;

&lt;p&gt;To do away with the exploring starts assumption we need to define a few more terms.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A policy $\pi$ is said to be &lt;strong&gt;$\varepsilon$-soft&lt;/strong&gt; if $\pi(a|s) &amp;gt; \frac{\varepsilon}{| A(s) |}$ for all $a \in A$ and $s \in S$.&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;$\varepsilon$-greedy&lt;/strong&gt; is a policy where all non-greedy actions are chosen with probability $\frac{\varepsilon}{| A(s) |}$, while the greedy policy is chosen with probability $1 - \varepsilon + \frac{\varepsilon}{| A(s) |}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;On-policy&lt;/strong&gt; methods evaluate and improve the policy that is used to make decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Off-policy&lt;/strong&gt; methods use one policy to make decisions while trying to evaluate and improve another policy.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;first-visit&lt;/strong&gt; MC method uses only the &lt;em&gt;return&lt;/em&gt; (not reward) after the &lt;em&gt;first&lt;/em&gt; occurance of a state (or state-action pair) when doing evaluation. An &lt;strong&gt;every-visit&lt;/strong&gt; method uses the &lt;em&gt;return&lt;/em&gt; after every occurance of a state (or state-action pair) when doing evaluation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An on-policy first-visit MC control algorithm for $\varepsilon$-soft policies is given below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/on_policy_eps_soft_mc_control.png&#34; alt=&#34;On-policy first-visit MC control&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;importance-sampling&#34;&gt;Importance sampling&lt;/h3&gt;

&lt;p&gt;This section will include more prediction and control methods, but is an important enough concept that it deserves its own section.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Temporal-Difference methods</title>
      <link>http://notes.spencerlyon.com/2016/06/29/temporal-difference-methods/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/temporal-difference-methods/</guid>
      <description>

&lt;p&gt;This is part 3 in the reinforcement learning for economists notes. For part 1 see &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt;. For a list of all entries in the series go &lt;a href=&#34;http://notes.spencerlyon.com/series/reinforcement-learning&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;temporal-difference-methods&#34;&gt;Temporal Difference methods&lt;/h2&gt;

&lt;p&gt;We continue our study of applying &lt;strong&gt;GPI&lt;/strong&gt; to the RL problem by looking now at &lt;strong&gt;temporal difference (TD)&lt;/strong&gt; methods.&lt;/p&gt;

&lt;h3 id=&#34;one-step-td-td-0&#34;&gt;One step TD (TD(0))&lt;/h3&gt;

&lt;!-- TODO: clean this exposition up -- it&#39;s not well written --&gt;

&lt;p&gt;Let&amp;rsquo;s begin our exploration of TD methods by considering the problem of evaluating or predicting the state-value function $V(s)$. The simplest TD algorithm will update $V(s)$ according to the following rule:&lt;/p&gt;

&lt;p&gt;$$V(s) \leftarrow V(s) + \alpha \left[G - V(S) \right],$$&lt;/p&gt;

&lt;p&gt;where $G$ is the return from state $s$. The term in the brackets is the difference between the actual reward in state $s$ ($G$) and the current estimate of that reward ($V(s)$) and is called the temporal difference.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>