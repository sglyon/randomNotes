<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mc on Random Notes</title>
    <link>http://notes.spencerlyon.com/tags/mc/</link>
    <description>Recent content in Mc on Random Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://notes.spencerlyon.com/tags/mc/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Monte Carlo methods</title>
      <link>http://notes.spencerlyon.com/2016/06/29/monte-carlo-methods/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/monte-carlo-methods/</guid>
      <description>

&lt;p&gt;This is part 2 in the reinforcement learning for economists notes. For part 1 see &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt;. For a list of all entries in the series go &lt;a href=&#34;http://notes.spencerlyon.com/series/reinforcement-learning&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;monte-carlo-methods&#34;&gt;Monte Carlo methods&lt;/h2&gt;

&lt;p&gt;As defined in &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt; &lt;em&gt;GPI&lt;/em&gt; encompasses the core ideas for all the RL algorithms in this book. One family of such algorithms is Monte Carlo (MC) methods. One distinguishing feature of these methods is that they can be implemented in a completely &lt;strong&gt;model-free&lt;/strong&gt; way. This means that to apply them we need to know nothing about the dynamics of the model; we simply need to be able to observe sequences of states, actions, and rewards.&lt;/p&gt;

&lt;h3 id=&#34;prediciton-methods&#34;&gt;Prediciton methods&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo prediciton&lt;/strong&gt; is a method for learning the state-value function for a given policy. The algorithm is given by:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/first_visit_mc_prediction.png&#34; alt=&#34;MC prediciton&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that the MC prediction algorithm does not update any estimates of the value function until &lt;em&gt;after&lt;/em&gt; the entire episode is realized. In this sense we would say that the method does not &lt;strong&gt;bootstrap&lt;/strong&gt;, meaning it does not use approximations of the value in other states to update the value in a particular state (value iteration is bootstrapping).&lt;/p&gt;

&lt;h3 id=&#34;control-methods&#34;&gt;Control methods&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo control&lt;/strong&gt; is an iterative algorithm where each iteration contains a Monte Carlo prediction step followed by a policy improvment step. The improvement step is simply done by making the policy greedy with the predicted value function.&lt;/p&gt;

&lt;p&gt;Some technical conditions must be satisfied to ensure convergence of this naive algorithm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;episodes have exploring starts (meaning that they don&amp;rsquo;t attempt to be greedy or optimal at the start)&lt;/li&gt;
&lt;li&gt;There are an infinite number of episodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are ok theoretically, but prohibative computationally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo ES&lt;/strong&gt; is an algorithm that does away with the assumption that you need an infinite number of episodes. It proceeds as&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/monte_carlo_es.png&#34; alt=&#34;MC ES&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that we don&amp;rsquo;t need the assumption of an infinite number of episodes because we don&amp;rsquo;t do a full policy evaluation each step.&lt;/p&gt;

&lt;p&gt;To do away with the exploring starts assumption we need to define a few more terms.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A policy $\pi$ is said to be &lt;strong&gt;$\varepsilon$-soft&lt;/strong&gt; if $\pi(a|s) &amp;gt; \frac{\varepsilon}{| A(s) |}$ for all $a \in A$ and $s \in S$.&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;$\varepsilon$-greedy&lt;/strong&gt; is a policy where all non-greedy actions are chosen with probability $\frac{\varepsilon}{| A(s) |}$, while the greedy policy is chosen with probability $1 - \varepsilon + \frac{\varepsilon}{| A(s) |}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;On-policy&lt;/strong&gt; methods evaluate and improve the policy that is used to make decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Off-policy&lt;/strong&gt; methods use one policy to make decisions while trying to evaluate and improve another policy.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;first-visit&lt;/strong&gt; MC method uses only the &lt;em&gt;return&lt;/em&gt; (not reward) after the &lt;em&gt;first&lt;/em&gt; occurance of a state (or state-action pair) when doing evaluation. An &lt;strong&gt;every-visit&lt;/strong&gt; method uses the &lt;em&gt;return&lt;/em&gt; after every occurance of a state (or state-action pair) when doing evaluation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An on-policy first-visit MC control algorithm for $\varepsilon$-soft policies is given below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://notes.spencerlyon.com/on_policy_eps_soft_mc_control.png&#34; alt=&#34;On-policy first-visit MC control&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;importance-sampling&#34;&gt;Importance sampling&lt;/h3&gt;

&lt;p&gt;This section will include more prediction and control methods, but is an important enough concept that it deserves its own section.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>