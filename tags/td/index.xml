<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Td on Random Notes</title>
    <link>http://notes.spencerlyon.com/tags/td/</link>
    <description>Recent content in Td on Random Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://notes.spencerlyon.com/tags/td/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Temporal-Difference methods</title>
      <link>http://notes.spencerlyon.com/2016/06/29/temporal-difference-methods/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://notes.spencerlyon.com/2016/06/29/temporal-difference-methods/</guid>
      <description>

&lt;p&gt;This is part 3 in the reinforcement learning for economists notes. For part 1 see &lt;a href=&#34;http://notes.spencerlyon.com/2016/06/29/reinforcement-learning-intro/&#34;&gt;Reinforcement Learning Intro&lt;/a&gt;. For a list of all entries in the series go &lt;a href=&#34;http://notes.spencerlyon.com/series/reinforcement-learning&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;temporal-difference-methods:da6883fa7b910696263cbcc1c81deb69&#34;&gt;Temporal Difference methods&lt;/h2&gt;

&lt;p&gt;We continue our study of applying &lt;strong&gt;GPI&lt;/strong&gt; to the RL problem by looking now at &lt;strong&gt;temporal difference (TD)&lt;/strong&gt; methods.&lt;/p&gt;

&lt;h3 id=&#34;one-step-td-td-0:da6883fa7b910696263cbcc1c81deb69&#34;&gt;One step TD (TD(0))&lt;/h3&gt;

&lt;!-- TODO: clean this exposition up -- it&#39;s not well written --&gt;

&lt;p&gt;Let&amp;rsquo;s begin our exploration of TD methods by considering the problem of evaluating or predicting the state-value function $V(s)$. The simplest TD algorithm will update $V(s)$ according to the following rule:&lt;/p&gt;

&lt;p&gt;$$V(s) \leftarrow V(s) + \alpha \left[G - V(S) \right],$$&lt;/p&gt;

&lt;p&gt;where $G$ is the return from state $s$. The term in the brackets is the difference between the actual reward in state $s$ ($G$) and the current estimate of that reward ($V(s)$) and is called the temporal difference.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>